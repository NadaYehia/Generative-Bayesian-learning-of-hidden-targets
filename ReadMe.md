Geneartive model for learning hidden rewarded locations in a square arena using Bayesian inference:

The code is optimised and paralleised to allow fast execution.

Our program starts with running **main.m** which comprises of the following :

  1) **Environment** class that has the attributes of the physical arena size and target locations & sizes, mode of interception to get a reward **intercept mode** and number of trials per target location 
     **blocks**. This class has static method that takes the target centeres and dimensions and outputs their respective corner coordinates. We envision to add further methods in this class that would handle 
     obstacles and its setup in the environment.
  2) **set control actions space** which constraint the action space (possible speeds and angles) given the physical limits of the arena.
  
  3) **simulate run** : given trajectory and the target location the agent will run and observe the outcome of their run at the home port.
     
  4)  **surprise**: this function detects changes in the environment (target locations changes) and reset the agent's prior to a uniform distribution if the outcome is too suprising.
     
  5)  **Bayes' update for action parameters** this function uses the action parameters of the generated trajectory and its observed outcome to estimate the likelihood function. The 
      likelihood function is generated by convolving the executed trajectory in the polar coordinates space with a 2D Gaussian kernel. This 2D Gaussian has a sigma fall off which represents our agent's belief 
      about the rewarding location size (consistency of the outcome around the locations that they actually ran). This smooth likelihood function then will be used to update the agent's belief about rewarding   
      location.   

  6) **Sampling_next_actions** this function represents the agent's sampling strategy after updating its posterior. Here we proposed 2 sampling algorithms, one can think of them as extremes of a continuum where at 
     one end it chooses the actions with the highest local reward probability as the next run actions (peak sampler), to the other end which chooses them proportional to the actions reward posterior.

  7) **Trajectory planner** this function is the core function for the agent generative model. the script ** Trajectory planner.m** takes anchor list from the sampler as input and finds the set of anchors heading 
       and distances that optimizes for path length and smoothness. This function then sample {x,y} points along the path connecting this set of optimized anchors, as well as calculating the {r,theta} polar 
       transformations of each point on this optimzied path. 
