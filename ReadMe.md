Generative model for trajectory learning in a Bayesian framework

The code is optimised and paralleised to allow fast execution. It takes ~1.5 minutes to simulate 50 agents. 

Our program starts with running **main.m** which comprises of the following 
  1) **Environment** class that has the attributes of the physical arena size and target locations & sizes, mode of interception to get a reward **intercept mode** and number of trials per target location 
     **blocks**. This class has static method that takes the target centeres and dimensions and outputs their respective corner coordinates. We envision to add further methods in this class that would handle 
     obstacles and its setup in the environment.
  2) **set control actions space** which constraint the action space (possible speeds and angles) given the physical limits of the arena.
  
  
  3) **connect actions with a smooth trajectory** this function is the core function for the agent generative model. It does the mapping of anchors from the action space to the x,y space, connect these anchors 
     coordinates using the agent generative model and finally maps back every x,y point on this trajectory to the action space using **convert xy to velocity and angle**; that is the necessary speed and heading 
     scale the agent will need to run from the port location (0,0) to each of these points.
4)  **Bayes' update for action parameters** this function uses the action parameters of the generated trajectory in (3) and the observed outcome after running it to estimate the likelihood function. The likelihood function is generated by convolving the executed trajectory in the action space with a Gaussian kernel. This Gaussian has a sigma fall off which reprents our agent's uncertainity on whether the neighboring actions to the executed ones might also lead to a reward/no reward. This smooth likelihood function then will be used to update the agent's belief about the correct action parameters.   
 5) **Sampling_next_actions** this function represents the agent's sampling strategy after updating its posterior. Here we proposed 2 sampling algorithms, one can think of them as extremes of a continuum where at one end it chooses the actions with the highest local reward probability as the next run actions (peak sampler), to the other end which chooses them proportional to the actions reward posterior.
